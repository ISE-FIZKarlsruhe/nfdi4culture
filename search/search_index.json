{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NFDI4Culture Ontology (CTO)","text":"<p>NFDI4Culture is the Consortium for Research Data on Material and Immaterial Cultural Heritage. Within NFDI4Culture, a Knowledge Graph has been developed and integrated with the Culture Information Portal with the goal to aggregate heterogeneous and isolated data from the research landscape focused on by NFDI4Culture and thereby increase the discoverability, interoperability and reusability of cultural heritage data. </p> <p>This repository provides the NFDI4Culture Ontology (CTO) and its documentation. CTO is a domain-specific ontology module developed within the Task Area 5 of the NFDI4Culture initiative. The ontology builds upon the NFDIcore mid-level ontology and is aligned with the Basic Formal Ontology (BFO) 2020 to ensure semantic interoperability and ontological rigor. CTO represents the research data of the NFDI4Culture community within a research data index, i.e. a single point of access to decentralized cultural heritage research resources. The ontology supports the integration of research metadata, harvested using a dedicated ETL pipeline. </p> <p>The ontology\u2019s latest version is always accessible at: cto.owl A generated list of resources via Widoco is available at:  https://nfdi.fiz-karlsruhe.de/4culture/ontology/ </p>"},{"location":"#ontology-metadata","title":"Ontology Metadata","text":"<ul> <li>Title: NFDI4Culture Ontology</li> <li>Abbreviation: cto</li> <li>Namespace: https://nfdi4culture.de/ontology</li> <li>Prefix: cto</li> <li>Language: OWL</li> <li>Repository: https://github.com/ISE-FIZKarlsruhe/nfdi4culture</li> <li>Previous version: CTO v2.2.0, February 08, 2024</li> <li>Creators: Tabea Tietz, J\u00f6rg Waitelonis, Oleksandra Bruns, Etienne Posthumus, Harald Sack</li> <li>Contributors: Linnaea S\u00f6hn, Jonatan Jalle Steller, Torsten Schrade</li> <li>Related project: NFDI4Culture</li> <li>Funding: Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the National Research Data Infrastructure \u2013 project number 441958017</li> <li>License: CC0 1.0</li> <li>Citation: Tabea Tietz, J\u00f6rg Waitelonis, Oleksandra Bruns, Etienne Posthumus, Harald Sack. NFDI4Culture Ontology (cto). Revision: v3.0.0. Retrieved from: https://github.com/ISE-FIZKarlsruhe/nfdi4culture</li> </ul>"},{"location":"cite/","title":"How to cite CTO","text":""},{"location":"contact/","title":"Contact","text":"<p>For questions or comments on the 4Culture knowledge graph laboratory, mail: Etienne Posthumus at ep@epoz.org</p> <p>For general enquiries on the consortium, please see the NFDI4Culture Helpdesk</p>"},{"location":"contact/#contributors","title":"Contributors","text":"<ul> <li> <p>Sasha Bruns https://orcid.org/0000-0002-8501-6700</p> </li> <li> <p>Heike Fliegl https://orcid.org/0000-0002-7541-115X</p> </li> <li> <p>Etienne Posthumus https://orcid.org/0000-0002-0006-7542</p> </li> <li> <p>Harald Sack https://orcid.org/0000-0001-7069-9804</p> </li> <li> <p>Linnaea S\u00f6hn https://orcid.org/0000-0001-8341-1187</p> </li> <li> <p>Torsten Schrade https://orcid.org/0000-0002-0953-2818</p> </li> <li> <p>Jonatan Jalle Steller https://orcid.org/0000-0002-5101-5275</p> </li> <li> <p>Tabea Tietz https://orcid.org/0000-0002-1648-1684</p> </li> <li> <p>Julia Tolksdorf https://orcid.org/0000-0002-0495-5897</p> </li> </ul>"},{"location":"contributing/","title":"How to contribute to CTO","text":""},{"location":"datasources/","title":"Misc Data Sources","text":"<p>This is a pin as a reminder for various datasources to consider.</p> <ul> <li> <p>NFDI4Culture CRIS IG, Hydra, RDF</p> </li> <li> <p>Marburg Bildindex, DDB query, LIDO</p> </li> <li> <p>Deutsche Fotothek, DDB query, LIDO</p> </li> <li> <p>Bayerische Staatsbibliothek, DDB, LIDO</p> </li> <li> <p>Bibliotheca Herziana, download from website, CGIF</p> </li> <li> <p>RADAR4Culture, download website, RDF</p> </li> <li> <p>Univ Heidelberg, OAI-PMH, LIDO</p> </li> </ul>"},{"location":"ddb/","title":"German Digital Library","text":"<p>https://www.deutsche-digitale-bibliothek.de/</p>"},{"location":"domain-examples/","title":"Domain Examples","text":"<p>The following modeling examples illustrate how research data can be represented using the CTO, with respect to the subject areas of performing arts, musicology, media studies, art history, and architecture. They also demonstrate the alignment of CTO with NFDIcore and the Basic Formal Ontology (BFO 2020).</p>"},{"location":"domain-examples/#legend","title":"Legend","text":""},{"location":"domain-examples/#the-performing-arts","title":"The Performing Arts","text":""},{"location":"domain-examples/#musicology","title":"Musicology","text":""},{"location":"factgrid/","title":"Factgrid","text":""},{"location":"factgrid/#a-database-for-historians","title":"A database for historians","text":"<p>https://factgrid.de/</p>"},{"location":"hydra/","title":"Hydra","text":"<p>Download data from sites that support the Hydra API</p>"},{"location":"lido/","title":"Lido","text":"<p>Lightweight Information Describing Objects is an XML harvesting schema.</p> <p>Be wary of any system that has the word \"lightweight\" in its name or description. It seldom is. Be that as it may, LIDO has been used as a standard in many cultural heritage contexts (mostly museums).</p>"},{"location":"nfdi-context/","title":"CTO in the Context of NFDI","text":"<p>NFDI4Culture is the Consortium for Research Data on Material and Immaterial Cultural Heritage. As a consortium within the framework of the German national research data infrastructure programme NFDI the goal of NFDI4Culture is the establishment of a information infrastructure for cultural heritage research data. This primarily concerns the subject areas architecture, art history, performing arts, musicology, and media sciences. </p> <p>Within NFDI4Culture, a Knowledge Graph has been developed and integrated with the Culture Information Portal with the goal to aggregate heterogeneous and isolated data from the research landscape focused on by NFDI4Culture and thereby increase the discoverability, interoperability and reusability of cultural heritage data. The NFDI4Culture Knowledge Graph acts as a single point of access to decentralized cultural heritage research resources. </p> <p>The NFDI4Culture Ontology (CTO) is a domain ontology developed within the Task Area 5 of the NFDI4Culture initiative. Its key objective is to facilitate the integration of cultural heritage research data into the NFDI4Culture Knowledge Graph, which is made available by means of the Culture Information Portal. </p>"},{"location":"nfdi-context/#modular-design-approach","title":"Modular Design Approach","text":"<p>NFDI consortia share overarching goals and concepts, including similarities in structure, governance, personnel and institutions, areas of expertise, services, and more. At the same time, each consortium also faces individual requirements and challenges, such as domain-specific standards, workflows, and methods for research data discovery. To facilitate the interoperability of research (meta)data both within and across consortia, and to enable cross-domain knowledge discovery while also addressing the specific needs of individual communities, a modular ontology design structure has been developed.</p> <p></p> <p>NFDIcore has been created as a mid-level ontology representing the overarching concepts of the NFDI community. This includes community structures, such as people, projects, and organizations, as well as the NFDI infrastructure, including services, software, APIs, guidelines, educational resources, and more. NFDIcore is compliant with the Basic Formal Ontology (BFO 2020). Consortium-specific NFDI modules capture tailored information and research data. CTO is a domain-specific extension that models the research data of the NFDI4Culture community.</p> <p>Next to CTO, further NFDI domain extensions are available below. Next to the listed ontolgies that are released and integrated in productive systems, further are currently under development.   </p> <ul> <li>NFDI-MatWerk (MWO) (Material Science Domain) </li> <li>NFDI4Memory (MEMO) (Historical Domain)</li> <li>NFDI4DataScience (NFDI4DSO) (Data Science Domain), for further information see also the EKAW 2024 Paper</li> </ul>"},{"location":"oai/","title":"Oai","text":"<p>Harvest repositories that offer data in OAI-PMH</p>"},{"location":"patterns/","title":"Patterns","text":"Query<pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX schema: &lt;http://schema.org/&gt;\nPREFIX cto: &lt;https://nfdi4culture.de/ontology/&gt;\nPREFIX nfdicore: &lt;https://nfdi.fiz-karlsruhe.de/ontology/&gt;\n\n\nSELECT DISTINCT ?person ?personLabel ?identifier\nWHERE {\n  ?person rdf:type nfdicore:NFDI_0000004 .\n  ?person rdfs:label ?personLabel .\n  ?person nfdicore:NFDI_0001006 ?identifier .\n} \n</code></pre>"},{"location":"queries/","title":"Queries","text":""},{"location":"queries/#term-count","title":"Term count","text":"<pre><code># counts  all defined terms in the triple store\n\nPREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\n\nSELECT (NOW() AS ?date) (xsd:integer(COUNT(?term)) AS ?count)\nWHERE {\n    ?term a &lt;http://schema.org/DefinedTerm&gt; .\n}\n</code></pre>"},{"location":"queries/#instance-count","title":"Instance count","text":"<pre><code>PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\n\nSELECT (NOW() AS ?date) ?type (xsd:integer(COUNT(?s)) AS ?count)\nWHERE {\n  ?subject a ?type .\n}\nGROUP BY ?type\nORDER BY ?type\n</code></pre>"},{"location":"queries/#resource-count","title":"Resource count","text":"<pre><code>PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\n\nSELECT (NOW() AS ?date) (xsd:integer(COUNT(DISTINCT ?resource)) AS ?count)\nWHERE {\n  ?resource ?p ?o .\n}\n</code></pre>"},{"location":"queries/#triple-count","title":"Triple count","text":"<pre><code># counts all triples in a triple store and returns\n# the total number together with a current timestamp\n\nPREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;\n\nSELECT (NOW() AS ?date) (xsd:integer(COUNT(*)) AS ?count) WHERE {\n   ?s ?p ?o\n}\n</code></pre>"},{"location":"rdg/","title":"Research Data Graph","text":""},{"location":"rdg/#ontology-sample-applications","title":"Ontology Sample Applications","text":"<p>Some examples of applying the RDG Ontology to items from architecture, art history, musicology, media studies and the performing arts.</p> zoom in/out with mouse scrollwheel or pinch-zoom"},{"location":"rdg/#architecture","title":"Architecture","text":"<pre><code>@prefix cto: &lt;https://nfdi4culture.de/ontology#&gt; .\n@prefix n4c: &lt;https://nfdi4culture.de/id/&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;http://schema.org/&gt; .\n@prefix nfdicore: &lt;https://nfdi.fiz-karlsruhe.de/ontology/&gt; .\n\n###  https://nfdi.fiz-karlsruhe.de/id/ark:/60538/14384775308556492773\n&lt;https://nfdi.fiz-karlsruhe.de/id/ark:/60538/14384775308556492773&gt; rdf:type owl:NamedIndividual ,\n                                                                            cto:Item ;\n                                                                   nfdicore:license &lt;https://www.deutsche-digitale-bibliothek.de/lizenzen/rv-fz&gt; ;\n                                                                   nfdicore:publisher n4c:E1876 ;\n                                                                   schema:image &lt;http://fotothek.biblhertz.it/bh/2048px/bh003277.jpg&gt; ,\n                                                                                             &lt;http://fotothek.biblhertz.it/bh/2048px/bh590824.jpg&gt; ,\n                                                                                             &lt;http://fotothek.biblhertz.it/bh/2048px/bh590962.jpg&gt; ;\n                                                                   schema:url &lt;http://foto.biblhertz.it/obj08016650&gt; ;\n                                                                   cto:gnd &lt;http://d-nb.info/gnd/129657379&gt; ,\n                                                                           &lt;http://d-nb.info/gnd/4045895-7&gt; ;\n                                                                   cto:itemType &lt;http://d-nb.info/gnd/4045895-7&gt; ;\n                                                                   cto:relatedPerson &lt;http://d-nb.info/gnd/129657379&gt; ;\n                                                                   cto:sourceFile &lt;https://api.deutsche-digitale-bibliothek.de/items/7YR6M735EGBHOOGHMZDE6HS4DUDOVEW2/source/record&gt; ;\n                                                                   cto:itemTypeLiteral \"Grabmal\"@de ;\n                                                                   rdfs:label \"Grabmonument des Kardinals Alano Coetivy\"@de .\n</code></pre>"},{"location":"rdg/#art-history","title":"Art History","text":"<pre><code>@prefix cto: &lt;https://nfdi4culture.de/ontology#&gt; .\n@prefix n4c: &lt;https://nfdi4culture.de/id/&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;http://schema.org/&gt; .\n@prefix nfdicore: &lt;https://nfdi.fiz-karlsruhe.de/ontology/&gt; .\n\n###  https://corpusvitrearum.de/id/F4485\n&lt;https://corpusvitrearum.de/id/F4485&gt; rdf:type owl:NamedIndividual ,\n                                               cto:Item ;\n                                      nfdicore:license &lt;https://creativecommons.org/licenses/by-nc/4.0/&gt; ;\n                                      nfdicore:publisher n4c:E1834 ;\n                                      schema:image &lt;https://corpusvitrearum.de/typo3temp/cvma/_processed_/pics/medium/4485.jpg&gt; ;\n                                      schema:url &lt;https://corpusvitrearum.de/id/F4485&gt; ;\n                                      cto:aat &lt;http://vocab.getty.edu/page/aat/300263722&gt; ;\n                                      cto:geonames &lt;http://sws.geonames.org/11427995&gt; ;\n                                      cto:iconclass &lt;https://iconclass.org/11G21&gt; ;\n                                      cto:itemType &lt;http://vocab.getty.edu/page/aat/300263722&gt; ;\n                                      cto:relatedLocation &lt;http://sws.geonames.org/11427995&gt; ;\n                                      cto:subjectConcept &lt;https://iconclass.org/11G21&gt; ;\n                                      cto:approximatePeriod \"vor 1397\" ;\n                                      cto:creationPeriod \"1380-01-01T00:00:00/1400-12-31T23:59:59\" ;\n                                      cto:itemTypeLiteral \"Glaskunst (Objektgattung)\"@de ;\n                                      rdfs:label \"Engel mit Dudelsack\"@de .\n</code></pre>"},{"location":"rdg/#media-studies","title":"Media studies","text":"<pre><code>@prefix cto: &lt;https://nfdi4culture.de/ontology#&gt; .\n@prefix n4c: &lt;https://nfdi4culture.de/id/&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;http://schema.org/&gt; .\n@prefix nfdicore: &lt;https://nfdi.fiz-karlsruhe.de/ontology/&gt; .\n\n###  https://nfdi.fiz-karlsruhe.de/id/ark:/60538/6077816772506314614\n&lt;https://nfdi.fiz-karlsruhe.de/id/ark:/60538/6077816772506314614&gt; rdf:type owl:NamedIndividual ,\n                                                                           schema:MediaObject ,\n                                                                           cto:Item ;\n                                                                  nfdicore:publisher &lt;https://d-nb.info/gnd/2002498-8&gt; ,\n                                                                                     n4c:E1979 ;\n                                                                  schema:image &lt;https://heidicon.ub.uni-heidelberg.de/api/v1/objects/uuid/a135cff7-832f-42d0-9385-8306bb297568/file/id/200688/file_version/name/small/disposition/inline&gt; ;\n                                                                  schema:url &lt;https://heidicon.ub.uni-heidelberg.de/detail/729667&gt; ;\n                                                                  cto:gnd &lt;https://d-nb.info/gnd/127825118&gt; ,\n                                                                          &lt;https://d-nb.info/gnd/4021845-4&gt; ,\n                                                                          &lt;https://d-nb.info/gnd/4029670-2&gt; ,\n                                                                          &lt;https://d-nb.info/gnd/4127793-4&gt; ,\n                                                                          &lt;https://d-nb.info/gnd/4135144-7&gt; ;\n                                                                  cto:iiifImageAPI &lt;https://heidicon.ub.uni-heidelberg.de/iiif/2/%3A200688&gt; ;\n                                                                  cto:itemType &lt;https://d-nb.info/gnd/4021845-4&gt; ,\n                                                                               &lt;https://d-nb.info/gnd/4029670-2&gt; ,\n                                                                               &lt;https://d-nb.info/gnd/4135144-7&gt; ;\n                                                                  cto:relatedLocation &lt;https://d-nb.info/gnd/4127793-4&gt; ;\n                                                                  cto:relatedPerson &lt;https://d-nb.info/gnd/127825118&gt; ;\n                                                                  cto:sourceFile &lt;https://heidicon.ub.uni-heidelberg.de/api/v1/plugin/base/oai/oai?verb=GetRecord&amp;metadataPrefix=lido&amp;identifier=oai:heidicon.ub.uni-heidelberg.de:a135cff7-832f-42d0-9385-8306bb297568&gt; ;\n                                                                  cto:creationPeriod \"1888\" ;\n                                                                  cto:itemTypeLiteral \"Fliegende Bl\u00e4tter\"@de ;\n                                                                  rdfs:label \"Gedichte eines Kleinst\u00e4dters\"@de .\n</code></pre>"},{"location":"rdg/#musicology","title":"Musicology","text":"<pre><code>@prefix cto: &lt;https://nfdi4culture.de/ontology#&gt; .\n@prefix n4c: &lt;https://nfdi4culture.de/id/&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;http://schema.org/&gt; .\n@prefix nfdicore: &lt;https://nfdi.fiz-karlsruhe.de/ontology/&gt; .\n@prefix mo: &lt;http://purl.org/ontology/mo/&gt; .\n\n###  https://rism.online/sources/201001959\n&lt;https://rism.online/sources/201001959&gt; rdf:type owl:NamedIndividual ,\n                                                 cto:Item ,\n                                                 schema:MusicComposition ;\n                                        nfdicore:license &lt;http://creativecommons.org/licenses/by/3.0/de&gt; ;\n                                        nfdicore:publisher &lt;https://rism.online/&gt; ;\n                                        mo:lyrics cto:So_sei_gegr\u00fc\u00dft_viel_tausendmal ;\n                                        schema:url &lt;https://rism.online/sources/201001959&gt; ;\n                                        cto:itemType &lt;https://rism.online/subjects/25227&gt; ,\n                                                     &lt;https://rism.online/subjects/25460&gt; ;\n                                        cto:relatedPerson &lt;https://rism.online/people/131206&gt; ;\n                                        cto:rism &lt;https://rism.online/people/131206&gt; ,\n                                                 &lt;https://rism.online/subjects/25227&gt; ,\n                                                 &lt;https://rism.online/subjects/25460&gt; ;\n                                        cto:approximatePeriod \"1890-1910 (19/20)\" ;\n                                        cto:creationPeriod \"1890-1910 (19/20)\" ;\n                                        cto:subjectConceptLiteral \"Coro\" ,\n                                                                  \"Coro S (2), Coro T, Coro B\" ;\n                                        rdfs:label \"Fr\u00fchlingsgru\u00df\"@de .\n\n\n###  https://rism.online/sources/201001959/incipits/1.1.1\n&lt;https://rism.online/sources/201001959/incipits/1.1.1&gt; rdf:type owl:NamedIndividual ,\n                                                                cto:Incipit ;\n                                                       cto:incipitOf &lt;https://rism.online/sources/201001959&gt; ;\n                                                       cto:clef \"G-2\" ;\n                                                       cto:keySignature \"xF\" ;\n                                                       cto:pattern \"4'D/8.6GB4AG/8.6''EC2'A/4.''D8'BAG/4BA\" ;\n                                                       cto:timeSignature \"3/4\" .\n</code></pre>"},{"location":"rdg/#performing-arts","title":"Performing Arts","text":"<pre><code>@prefix cto: &lt;https://nfdi4culture.de/ontology#&gt; .\n@prefix n4c: &lt;https://nfdi4culture.de/id/&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix schema: &lt;http://schema.org/&gt; .\n@prefix nfdicore: &lt;https://nfdi.fiz-karlsruhe.de/ontology/&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n\n###  http://slod.fiz-karlsruhe.de/labw-2-2599390\n&lt;http://slod.fiz-karlsruhe.de/labw-2-2599390&gt; rdf:type owl:NamedIndividual ,\n                                                       cto:Item ;\n                                              nfdicore:license &lt;https://creativecommons.org/licenses/by/2.0/&gt; ;\n                                              nfdicore:publisher &lt;http://slod.fiz-karlsruhe.de/labw-2-2599390&gt; ;\n                                              schema:image &lt;http://slod.fiz-karlsruhe.de/images/slod/2-2599390-1.jpg&gt; ,\n                                                                        &lt;http://slod.fiz-karlsruhe.de/images/slod/2-2599390-2.jpg&gt; ,\n                                                                        &lt;http://slod.fiz-karlsruhe.de/images/slod/2-2599390-3.jpg&gt; ;\n                                              schema:url &lt;https://slod.fiz-karlsruhe.de/labw-2-2599390&gt; ;\n                                              cto:gnd &lt;http://d-nb.info/gnd/4316770-6&gt; ,\n                                                      &lt;https://d-nb.info/gnd/116463619&gt; ,\n                                                      &lt;https://d-nb.info/gnd/118613723&gt; ;\n                                              cto:relatedPerson &lt;http://www.wikidata.org/entity/Q55638867&gt; ,\n                                                                &lt;http://www.wikidata.org/entity/Q692&gt; ,\n                                                                &lt;https://d-nb.info/gnd/116463619&gt; ,\n                                                                &lt;https://d-nb.info/gnd/118613723&gt; ;\n                                              cto:subjectConcept &lt;http://d-nb.info/gnd/4316770-6&gt; ,\n                                                                 &lt;http://www.wikidata.org/entity/Q221211&gt; ;\n                                              cto:wikidata &lt;http://www.wikidata.org/entity/Q221211&gt; ,\n                                                           &lt;http://www.wikidata.org/entity/Q55638867&gt; ,\n                                                           &lt;http://www.wikidata.org/entity/Q692&gt; ;\n                                              cto:creationDate \"1923-03-11\"^^xsd:date ;\n                                              cto:subjectConceptLiteral \"Schauspiel\"@de ;\n                                              rdfs:label \"Was ihr wollt (William Shakespeare)\"@en .\n</code></pre>"},{"location":"references/","title":"References and Publications","text":""},{"location":"references/#references","title":"References","text":"<p>[1] ISO/IEC 21838-2:2021, Information technology \u2014 Top-level ontologies (TLO) Part 2: Basic Formal Ontology (BFO), https://www.iso.org/standard/74572.html [2] NFDIcore ontology (v3): https://github.com/ISE-FIZKarlsruhe/nfdicore/tree/main [3] Information Artifact Ontology (IAO): https://github.com/information-artifact-ontology/IAO [4] Software Ontology (SWO): https://github.com/allysonlister/swo [5] Data Catalog Vocabulary (DCAT): https://www.w3.org/TR/vocab-dcat-3/ [6] schema.org vocabulary: https://schema.org/ [7] skos: https://www.w3.org/2004/02/skos/ </p>"},{"location":"references/#publications","title":"Publications","text":"<p>CTOv3.0 has not yet been described and published in peer reviewed venues. However, please find publications related to NFDI4Culture, the knowledge graph and data ingestion workflow below. The list will be updated. </p> <ul> <li>L. S\u00f6hn, T. Tietz, J. J. Steller, P. Kehrein, A. B\u00fcttner, E. Posthumus, O. Bruns, J. Gr\u00fcnew\u00e4lder, J. H\u00f6rnschemeyer, C. Sander, V. Grund, H. Fliegl, H. Sack, T. Schrade, NFDI4Culture Integration Stories: Bridging Gaps Between Isolated Research Resources at the Digital Humanities Conference in Lisbon, Portugal, July 2025 (to be published)</li> <li>O. Bruns, T. Tietz, L. S\u00f6hn, J. J. Steller, S. R. Ondraszek, E. Posthumus, T. Schrade, H. Sack, What\u2019s Cooking in the NFDI4Culture Kitchen? A KG-based Research Data Integration Workflow, in Proc. of 4th Workshop on Metadata and Research (objects) Management for Linked Open Science - DaMaLOS 2024, 2024</li> <li>O. Bruns, L.S\u00f6hn,T. Tietz, J. J. Steller, E. Posthumus, T. Schrade, H. Sack, Gotta Catch\u2019em All: From Data Silos to a Knowledge Graph, Proc. of 21st Extended Semantic Web Conference (ESWC 2024), Poster &amp; Demos, 2024. </li> <li>J. J. Steller, L. C. S\u00f6hn, J. Tolksdorf, O. Bruns, T. Tietz, E. Posthumus, H. Fliegl, S. Pittroff, H. Sack, T. Schrade: Communities, Harvesting, and CGIF: Building the Research Data Graph at NFDI4Culture. In Proc. of the DHd 2024, 10. Tagung des Verbands Digital Humanities im deutschsprachigen Raum</li> <li>T. Tietz, O. Bruns, L. S\u00f6hn, J. Tolksdorf, E. Posthumus, J.J. Steller, H. Fliegl, E. Norouzi, J. Waitelonis, T. Schrade, H. Sack: From Floppy Disks to 5-Star LOD: FAIR Research Infrastructure for NFDI4Culture. In Proc. of 3rd Workshop on Metadata and Research (objects) Management for Linked Open Science - DaMaLOS 2023</li> </ul>"},{"location":"structure/","title":"Ontology Scope and Structure","text":"<p>The following section describes the main scope of CTO, the ontologies reused to ensure cross-domain interoperability, and its core structure. </p>"},{"location":"structure/#scope","title":"Scope","text":"<p>The main scope of CTO is the representation of cultural heritage research data within a data index, i.e. a single point of access to decentralized cultural heritage research resources. The ontology supports the integration of research (meta)data harvested through a dedicated ETL pipeline. Its main focus is the creation of a lightweight index of cultural heritage research data provided by the culture community, including but not limited to the subject areas of musicology, performing arts, media studies, architecture, and art history.</p> <p>The key aspects of CTO concern:</p> <ul> <li>The consortium and its infrastructure: persons and organizations involved in research processes, services, guidelines, standards, and events</li> <li>The content of cultural heritage research data: cultural heritage objects, persons, locations and events referenced in the provided data, associated media, references to external vocabularies used for the identification and classification of the provided research data</li> <li>Access and reuse: legal statements, contact persons, standards, and export formats </li> </ul>"},{"location":"structure/#reused-ontologies","title":"Reused Ontologies","text":"<p>Building CTO based on standardized upper-level ontologies provides a well-defined and standardized semantic structure, ensuring clear definitions of entities and their relationships. These ontologies enhance interoperability across different domains, support ontology development, and promote consistency in knowledge representation. The following ontologies have been reused in CTO:</p> <ul> <li>Basic Formal Ontology (BFO 2020): BFO was selected as the top-level ontology due to its well-structured design, broad applicability, and ability to integrate with various ontologies. As a foundational framework, BFO provides abstract, cross-domain semantic structures, ensuring comprehensive integration and compliance with top-level ontology standards. Its adherence to the ISO/IEC 21838-2 standards [1] further enhances interoperability in developed ontologies, facilitating knowledge representation, data exchange, and interdisciplinary collaboration.</li> <li>NFDIcore ontology v3:: As a mid-level ontology, NFDIcore plays a central role in structuring and integrating research data across consortia [2]. NFDIcore features a modular structure for improved interoperability among NFDI consortia. It provides a shared vocabulary that represents both the organizational structure of the NFDI and the diverse datasets contributed by project partners. The ontology encompasses key concepts such as organizations, consortia, projects, datasets, research outputs, geographical locations, and technical standards. These structured representations enable efficient data management, integration, and reuse across disciplines. </li> <li>Information Artefact Ontology (IAO): The Information Artefact Ontology was partially reused [3]. The most central IAO class supporting CTO\u2019s alignment with BFO is <code>iao:information content entity</code>. Since IAO does not yet fully support BFO 2020, certain relevant concepts could not be reused directly. In these cases, NFDIcore-specific classes\u2014such as <code>dataset</code>, <code>document</code>, and <code>identifier</code>\u2014were introduced to fill the gaps.</li> <li>Schema.org: In CTO, schema.org was particularly used for describing creative works and to ensure the alignment with the Culture Information Portal\u2019s existing data model [6]. </li> <li>Further reused ontologies and vocabularies include the Software Ontology (SWO)[4], Data Catalog Vocabulary (DCAT)[5], and skos[7].</li> </ul>"},{"location":"structure/#core-structure","title":"Core Structure","text":"<p>The core structure of CTO consists of four main elements. <code>schema:DataFeed</code> represents a data feed in the Research Information Graph (RIG). This data feed is created in the Culture Information Portal using TYPO3 and its LOD extension. The metadata associated with the data feed include contact persons, export formats, licenses, and related projects and organizations. For each item (<code>cto: source item</code>) in the data feed, a permanent ARK (<code>schema:DataFeedItem</code>) is created as its stable reference entity. The <code>schema:DataFeedItem</code> does not contain content-related information about the source item, aside from a license issued by NFDI4Culture and the creation and modification dates. This stable reference entity functions as a persistent identifier in the knowledge graph, remaining valid even if the content of the source item is changed or deleted. The main content-related metadata are associated with <code>cto: source item</code>. This includes associated media, related entities and their identifiers in external vocabularies, temporal data, and subject-area-specific metadata, such as musical incipits. Furthermore, it is possible to express which real-world entity <code>cto: source item</code> is about. These entities could be sculptures, buildings, persons, books, etc. However, since this information is often not provided in the research data, this aspect currently serves as a proof of concept and is only materialized when clearly stated by the data providers.</p>"},{"location":"odk-workflows/","title":"Default ODK Workflows","text":"<ul> <li>Daily Editors Workflow</li> <li>Release Workflow</li> <li>Manage your ODK Repository</li> <li>Setting up Docker for ODK</li> <li>Imports management</li> <li>Managing the documentation</li> <li>Managing your Automated Testing</li> </ul>"},{"location":"odk-workflows/ContinuousIntegration/","title":"Introduction to Continuous Integration Workflows with ODK","text":"<p>Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding  this to your configuration file (src/ontology/cto-odk.yaml):</p> <pre><code>ci:\n  - github_actions\n</code></pre> <p>When updateing your repo, you will notice a new file being added: <code>.github/workflows/qc.yml</code>.</p> <p>This file contains your CI logic, so if you need to change, or add anything, this is the place!</p> <p>Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding  this to your configuration file (src/ontology/cto-odk.yaml):</p> <pre><code>ci:\n  - gitlab-ci\n</code></pre> <p>This will add a file called <code>.gitlab-ci.yml</code> in the root of your repo.</p>"},{"location":"odk-workflows/EditorsWorkflow/","title":"Editors Workflow","text":"<p>The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows:</p> <ol> <li>Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns.</li> <li>Completely automated data pipeline (GitHub Actions)</li> <li>DROID workflow</li> </ol> <p>This document only covers the first editing workflow, but more will be added in the future</p>"},{"location":"odk-workflows/EditorsWorkflow/#local-editing-workflow","title":"Local editing workflow","text":"<p>Workflow requirements:</p> <ul> <li>git</li> <li>github</li> <li>docker</li> <li>editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc</li> </ul>"},{"location":"odk-workflows/EditorsWorkflow/#1-create-issue","title":"1. Create issue","text":"<p>Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change.</p>"},{"location":"odk-workflows/EditorsWorkflow/#2-update-main-branch","title":"2. Update main branch","text":"<p>In your local environment (e.g. your laptop), make sure you are on the <code>main</code> (prev. <code>master</code>) branch and ensure that you have all the upstream changes, for example:</p> <pre><code>git checkout main\ngit pull\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#3-create-feature-branch","title":"3. Create feature branch","text":"<p>Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases)</p> <p>On your command line, this looks like this:</p> <pre><code>git checkout -b issue23removeprocess\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#4-perform-edit","title":"4. Perform edit","text":"<p>Using your editor of choice, perform the intended edit. For example:</p> <p>Prot\u00e9g\u00e9</p> <ol> <li>Open <code>src/ontology/cto-edit.owl</code> in Prot\u00e9g\u00e9</li> <li>Make the change</li> <li>Save the file</li> </ol> <p>TextEdit</p> <ol> <li>Open <code>src/ontology/cto-edit.owl</code> in TextEdit (or Sublime, Atom, Vim, Nano)</li> <li>Make the change</li> <li>Save the file</li> </ol> <p>Consider the following when making the edit.</p> <ol> <li>According to our development philosophy, the only places that should be manually edited are:<ul> <li><code>src/ontology/cto-edit.owl</code></li> <li>Any ROBOT templates you chose to use (the TSV files only)</li> <li>Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns)</li> <li>components (anything in <code>src/ontology/components</code>), see here.</li> </ul> </li> <li>Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere.</li> <li>Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully.</li> </ol>"},{"location":"odk-workflows/EditorsWorkflow/#4-check-the-git-diff","title":"4. Check the Git diff","text":"<p>This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking.</p> <p>In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line:</p> <pre><code>git status\ngit diff\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#5-quality-control","title":"5. Quality control","text":"<p>Now it's time to run your quality control checks. This can either happen locally (5a) or through your continuous integration system (7/5b).</p>"},{"location":"odk-workflows/EditorsWorkflow/#5a-local-testing","title":"5a. Local testing","text":"<p>If you chose to run your test locally:</p> <p><pre><code>sh run.sh make IMP=false test\n</code></pre> This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add <code>PAT=false</code> to skip the potentially lengthy process of rebuilding the patterns.</p> <pre><code>sh run.sh make IMP=false PAT=false test\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#6-pull-request","title":"6. Pull request","text":"<p>When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example:</p> <pre><code>git add NAMEOFCHANGEDFILES\ngit commit -m \"Added biological process term #12\"\ngit push -u origin issue23removeprocess\n</code></pre> <p>Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls</p> <p>There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: <code>see #23</code> to link to a related ticket, or <code>fixes #23</code> if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change).</p>"},{"location":"odk-workflows/EditorsWorkflow/#75b-continuous-integration-testing","title":"7/5b. Continuous Integration Testing","text":"<p>If you didn't run and local quality control checks (see 5a), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions</p> <p>More on how to set this up here. Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red.</p>"},{"location":"odk-workflows/EditorsWorkflow/#8-community-review","title":"8. Community review","text":"<p>Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this.</p> <p>This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out!</p>"},{"location":"odk-workflows/EditorsWorkflow/#9-merge-and-cleanup","title":"9. Merge and cleanup","text":"<p>When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request.</p>"},{"location":"odk-workflows/EditorsWorkflow/#10-changelog-optional","title":"10. Changelog (Optional)","text":"<p>It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).</p>"},{"location":"odk-workflows/ManageAutomatedTest/","title":"Managing your automated testing","text":""},{"location":"odk-workflows/ManageAutomatedTest/#constraint-violation-checks","title":"Constraint violation checks","text":"<p>We can define custom checks using SPARQL. SPARQL queries define bad modelling patterns (missing labels, misspelt URIs, and many more) in the ontology. If these queries return any results, then the build will fail. Custom checks are designed to be run as part of GitHub Actions Continuous Integration testing, but they can also run locally.</p>"},{"location":"odk-workflows/ManageAutomatedTest/#steps-to-add-a-constraint-violation-check","title":"Steps to add a constraint violation check:","text":"<ol> <li>Add the SPARQL query in <code>src/sparql</code>. The name of the file should end with <code>-violation.sparql</code>. Please give a name that helps to understand which violation the query wants to check.</li> <li>Add the name of the new file to odk configuration file <code>src/ontology/uberon-odk.yaml</code>:<ol> <li>Include the name of the file (without the <code>-violation.sparql</code> part) to the list inside the key <code>custom_sparql_checks</code> that is inside <code>robot_report</code> key.</li> <li> <p>If the <code>robot_report</code> or <code>custom_sparql_checks</code> keys are not available, please add this code block to the end of the file.</p> <p><pre><code>  robot_report:\n    release_reports: False\n    fail_on: ERROR\n    use_labels: False\n    custom_profile: True\n    report_on:\n      - edit\n    custom_sparql_checks:\n      - name-of-the-file-check\n</code></pre> 3. Update the repository so your new SPARQL check will be included in the QC.</p> </li> </ol> </li> </ol> <pre><code>sh run.sh make update_repo\n</code></pre>"},{"location":"odk-workflows/ManageDocumentation/","title":"Updating the Documentation","text":"<p>The documentation for CTO is managed in two places (relative to the repository root):</p> <ol> <li>The <code>docs</code> directory contains all the files that pertain to the content of the documentation (more below)</li> <li>the <code>mkdocs.yaml</code> file contains the documentation config, in particular its navigation bar and theme.</li> </ol> <p>The documentation is hosted using GitHub pages, on a special branch of the repository (called <code>gh-pages</code>). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually. All changes to the docs happen inside the <code>docs</code> directory on the <code>main</code> branch.</p>"},{"location":"odk-workflows/ManageDocumentation/#editing-the-docs","title":"Editing the docs","text":""},{"location":"odk-workflows/ManageDocumentation/#changing-content","title":"Changing content","text":"<p>All the documentation is contained in the <code>docs</code> directory, and is managed in Markdown. Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow:</p> <ol> <li>Open the <code>.md</code> file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT: Do not edit any files in the <code>docs/odk-workflows/</code> directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker.</li> <li>Perform the edit and save the file</li> <li>Commit the file to a branch, and create a pull request as usual. </li> <li>If your development team likes your changes, merge the docs into main branch.</li> <li>Deploy the documentation (see below)</li> </ol>"},{"location":"odk-workflows/ManageDocumentation/#deploy-the-documentation","title":"Deploy the documentation","text":"<p>The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps:</p> <ol> <li>In your terminal, navigate to the edit directory of your ontology, e.g.:    <pre><code>cd cto/src/ontology\n</code></pre></li> <li>Now you are ready to build the docs as follows:    <pre><code>sh run.sh make update_docs\n</code></pre> Mkdocs now sets off to build the site from the markdown pages. You will be asked to<ul> <li>Enter your username</li> <li>Enter your password (see here for using GitHub access tokens instead)   IMPORTANT: Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens!</li> </ul> </li> </ol> <p>If everything was successful, you will see a message similar to this one:</p> <p><pre><code>INFO    -  Your documentation should shortly be available at: https://ISE-FIZKarlsruhe.github.io/nfdi4culture/ \n</code></pre> 3. Just to double check, you can now navigate to your documentation pages (usually https://ISE-FIZKarlsruhe.github.io/nfdi4culture/).     Just make sure you give GitHub 2-5 minutes to build the pages!</p>"},{"location":"odk-workflows/ReleaseWorkflow/","title":"The release workflow","text":"<p>The release workflow recommended by the ODK is based on GitHub releases and works as follows:</p> <ol> <li>Run a release with the ODK</li> <li>Review the release</li> <li>Merge to main branch</li> <li>Create a GitHub release</li> </ol> <p>These steps are outlined in detail in the following.</p>"},{"location":"odk-workflows/ReleaseWorkflow/#run-a-release-with-the-odk","title":"Run a release with the ODK","text":"<p>Preparation:</p> <ol> <li>Ensure that all your pull requests are merged into your main (master) branch</li> <li>Make sure that all changes to main are committed to GitHub (<code>git status</code> should say that there are no modified files)</li> <li>Locally make sure you have the latest changes from main (<code>git pull</code>)</li> <li>Checkout a new branch (e.g. <code>git checkout -b release-2021-01-01</code>)</li> <li>You may or may not want to refresh your imports as part of your release strategy (see here)</li> <li>Make sure you have the latest ODK installed by running <code>docker pull obolibrary/odkfull</code></li> </ol> <p>To actually run the release, you:</p> <ol> <li>Open a command line terminal window and navigate to the src/ontology directory (<code>cd cto/src/ontology</code>)</li> <li>Run release pipeline:<code>sh run.sh make prepare_release -B</code>. Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI.</li> <li>If everything went well, you should see the following output on your machine: <code>Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab</code>.</li> </ol> <p>This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo).</p>"},{"location":"odk-workflows/ReleaseWorkflow/#review-the-release","title":"Review the release","text":"<ol> <li>(Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (cto.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: <ol> <li>Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly.</li> <li>Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. </li> </ol> </li> <li>Commit your changes to the branch and make a pull request</li> <li>In your GitHub pull request, review the following three files in detail (based on our experience):<ol> <li><code>cto.obo</code> - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review!</li> <li><code>cto-base.owl</code> - this reflects the asserted axioms in your ontology that you have actually edited.</li> <li>Ideally also take a look at <code>cto-full.owl</code>, which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large.</li> </ol> </li> <li>Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR!</li> </ol>"},{"location":"odk-workflows/ReleaseWorkflow/#merge-the-main-branch","title":"Merge the main branch","text":"<p>Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished).</p>"},{"location":"odk-workflows/ReleaseWorkflow/#create-a-github-release","title":"Create a GitHub release","text":"<ol> <li>Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/ISE-FIZKarlsruhe/nfdi4culture/releases). Then click \"Draft new release\"</li> <li>As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the <code>cto.obo</code> file and check the <code>data-version:</code> property. The date needs to be prefixed with a <code>v</code>, so, for example <code>v2020-02-06</code>.</li> <li>You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions.</li> <li>Click \"Publish release\". Done.</li> </ol>"},{"location":"odk-workflows/ReleaseWorkflow/#debugging-typical-ontology-release-problems","title":"Debugging typical ontology release problems","text":""},{"location":"odk-workflows/ReleaseWorkflow/#problems-with-memory","title":"Problems with memory","text":"<p>When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here.</p>"},{"location":"odk-workflows/ReleaseWorkflow/#problems-when-using-obo-format-based-tools","title":"Problems when using OBO format based tools","text":"<p>Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations (<code>def</code>, <code>comment</code>) which are illegal in OBO. Here is an example recipe of how to deal with such a problem:</p> <ol> <li>If you get a message like <code>make: *** [cl.Makefile:84: oort] Error 255</code> you might have a OORT error. </li> <li>To debug this, in your terminal enter <code>sh run.sh make IMP=false PAT=false oort -B</code> (assuming you are already in the ontology folder in your directory) </li> <li>This should show you where the error is in the log (eg multiple different definitions)  WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE</li> <li>Open <code>cto-edit.owl</code> in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save.  *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in.</li> <li>Rerun <code>sh run.sh make IMP=false PAT=false oort -B</code> and if it all passes, commit your changes to a branch and make a pull request as usual.</li> </ol>"},{"location":"odk-workflows/RepoManagement/","title":"Managing your ODK repository","text":""},{"location":"odk-workflows/RepoManagement/#updating-your-odk-repository","title":"Updating your ODK repository","text":"<p>Your ODK repositories configuration is managed in <code>src/ontology/cto-odk.yaml</code>. The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository:</p> <pre><code>sh run.sh make update_repo\n</code></pre> <p>There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here.</p> <p>NOTE for Windows users:</p> <p>You may get a cryptic failure such as <code>Set Illegal Option -</code> if the update script located in <code>src/scripts/update_repo.sh</code>  was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can  click on Edit-&gt;EOL Conversion-&gt;Unix LF to change this.</p>"},{"location":"odk-workflows/RepoManagement/#managing-imports","title":"Managing imports","text":"<p>You can use the update repository workflow described on this page to perform the following operations to your imports:</p> <ol> <li>Add a new import</li> <li>Modify an existing import</li> <li>Remove an import you no longer want</li> <li>Customise an import</li> </ol> <p>We will discuss all these workflows in the following.</p>"},{"location":"odk-workflows/RepoManagement/#add-new-import","title":"Add new import","text":"<p>To add a new import, you first edit your odk config as described above, adding an <code>id</code> to the <code>product</code> list in the <code>import_group</code> section (for the sake of this example, we assume you already import RO, and your goal is to also import GO):</p> <pre><code>import_group:\n  products:\n    - id: ro\n    - id: go\n</code></pre> <p>Note: our ODK file should only have one <code>import_group</code> which can contain multiple imports (in the <code>products</code> section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here. To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps:</p> <ol> <li>Add an import statement to your <code>src/ontology/cto-edit.owl</code> file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows:     <pre><code>...\nOntology(&lt;https://nfdi4culture.de/ontology/cto.owl&gt;\nImport(&lt;https://nfdi4culture.de/ontology/cto/imports/ro_import.owl&gt;)\nImport(&lt;https://nfdi4culture.de/ontology/cto/imports/go_import.owl&gt;)\n...\n</code></pre></li> <li>Add your imports redirect to your catalog file <code>src/ontology/catalog-v001.xml</code>, for example:     <pre><code>&lt;uri name=\"http://purl.obolibrary.org/obo/cto/imports/go_import.owl\" uri=\"imports/go_import.owl\"/&gt;\n</code></pre></li> <li>Test whether everything is in order:<ol> <li>Refresh your import</li> <li>Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported.</li> </ol> </li> </ol> <p>Note: The catalog file <code>src/ontology/catalog-v001.xml</code> has one purpose: redirecting  imports from URLs to local files. For example, if you have</p> <pre><code>Import(&lt;http://purl.obolibrary.org/obo/cto/imports/go_import.owl&gt;)\n</code></pre> <p>in your editors file (the ontology) and</p> <pre><code>&lt;uri name=\"https://nfdi4culture.de/ontology/cto/imports/go_import.owl\" uri=\"imports/go_import.owl\"/&gt;\n</code></pre> <p>in your catalog, tools like <code>robot</code> or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL <code>http://purl.obolibrary.org/obo/cto/imports/go_import.owl</code> to the local file <code>imports/go_import.owl</code> (which is in your <code>src/ontology</code> directory).</p>"},{"location":"odk-workflows/RepoManagement/#modify-an-existing-import","title":"Modify an existing import","text":"<p>If you simply wish to refresh your import in light of new terms, see here. If you wish to change the type of your module see section \"Customise an import\".</p>"},{"location":"odk-workflows/RepoManagement/#remove-an-existing-import","title":"Remove an existing import","text":"<p>To remove an existing import, perform the following steps:</p> <ol> <li>remove the import declaration from your <code>src/ontology/cto-edit.owl</code>.</li> <li>remove the id from your <code>src/ontology/cto-odk.yaml</code>, eg. <code>- id: go</code> from the list of <code>products</code> in the <code>import_group</code>.</li> <li>run update repo workflow</li> <li>delete the associated files manually:<ul> <li><code>src/imports/go_import.owl</code></li> <li><code>src/imports/go_terms.txt</code></li> </ul> </li> <li>Remove the respective entry from the <code>src/ontology/catalog-v001.xml</code> file.</li> </ol>"},{"location":"odk-workflows/RepoManagement/#customise-an-import","title":"Customise an import","text":"<p>By default, an import module extracted from a source ontology will be a SLME module, see here. There are various options to change the default.</p> <p>The following change to your repo config (<code>src/ontology/cto-odk.yaml</code>) will switch the go import from an SLME module to a simple ROBOT filter module:</p> <pre><code>import_group:\n  products:\n    - id: ro\n    - id: go\n      module_type: filter\n</code></pre> <p>A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the <code>filter</code> module does  not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This  behaviour can be changed by adding additional base IRIs as follows:</p> <pre><code>import_group:\n  products:\n    - id: go\n      module_type: filter\n      base_iris:\n        - http://purl.obolibrary.org/obo/GO_\n        - http://purl.obolibrary.org/obo/CL_\n        - http://purl.obolibrary.org/obo/BFO\n</code></pre> <p>If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config (<code>src/ontology/cto-odk.yaml</code>):</p> <pre><code>import_group:\n  products:\n    - id: ro\n    - id: go\n      module_type: custom\n</code></pre> <p>Now add a new goal in your custom Makefile (<code>src/ontology/cto.Makefile</code>, not <code>src/ontology/Makefile</code>).</p> <pre><code>imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt\n    if [ $(IMP) = true ]; then $(ROBOT) query  -i $&lt; --update ../sparql/preprocess-module.ru \\\n        extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\\n        query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\\n        annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n</code></pre> <p>Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only:</p> <pre><code>extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\\n</code></pre> <p>to another ROBOT pipeline.</p>"},{"location":"odk-workflows/RepoManagement/#add-a-component","title":"Add a component","text":"<p>A component is an import which belongs to your ontology, e.g. is managed by  you and your team. </p> <ol> <li>Open <code>src/ontology/cto-odk.yaml</code></li> <li>If you dont have it yet, add a new top level section <code>components</code></li> <li>Under the <code>components</code> section, add a new section called <code>products</code>.  This is where all your components are specified</li> <li>Under the <code>products</code> section, add a new component, e.g. <code>- filename: mycomp.owl</code></li> </ol> <p>Example</p> <pre><code>components:\n  products:\n    - filename: mycomp.owl\n</code></pre> <p>When running <code>sh run.sh make update_repo</code>, a new file <code>src/ontology/components/mycomp.owl</code> will  be created which you can edit as you see fit. Typical ways to edit:</p> <ol> <li>Using a ROBOT template to generate the component (see below)</li> <li>Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor</li> <li>Providing a <code>components/mycomp.owl:</code> make target in <code>src/ontology/cto.Makefile</code> and provide a custom command to generate the component<ul> <li><code>WARNING</code>: Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue).</li> </ul> </li> <li>Providing an additional attribute for the component in <code>src/ontology/cto-odk.yaml</code>, <code>source</code>, to specify that this component should simply be downloaded from somewhere on the web.</li> </ol>"},{"location":"odk-workflows/RepoManagement/#adding-a-new-component-based-on-a-robot-template","title":"Adding a new component based on a ROBOT template","text":"<p>Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps:</p> <ol> <li>Open <code>src/ontology/cto-odk.yaml</code>.</li> <li>Make sure that <code>use_templates: TRUE</code> is set in the global project options. You should also make sure that <code>use_context: TRUE</code> is set in case you are using prefixes in your templates that are not known to <code>robot</code>, such as <code>OMOP:</code>, <code>CPONT:</code> and more. All non-standard prefixes you are using should be added to <code>config/context.json</code>.</li> <li>Add another component to the <code>products</code> section.</li> <li>To activate this component to be template-driven, simply say: <code>use_template: TRUE</code>. This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. <code>run.bat make recreate-mycomp</code>).</li> <li>If you want to use more than one component, use the <code>templates</code> field to add as many template names as you wish. ODK will look for them in the <code>src/templates</code> directory.</li> <li>Advanced: If you want to provide additional processing options, you can use the <code>template_options</code> field. This should be a string with option from robot template. One typical example for additional options you may want to provide is <code>--add-prefixes config/context.json</code> to ensure the prefix map of your context is provided to <code>robot</code>, see above.</li> </ol> <p>Example:</p> <pre><code>components:\n  products:\n    - filename: mycomp.owl\n      use_template: TRUE\n      template_options: --add-prefixes config/context.json\n      templates:\n        - template1.tsv\n        - template2.tsv\n</code></pre> <p>Note: if your mirror is particularly large and complex, read this ODK recommendation.</p>"},{"location":"odk-workflows/RepositoryFileStructure/","title":"Repository structure","text":"<p>The main kinds of files in the repository:</p> <ol> <li>Release files</li> <li>Imports</li> <li>Components</li> </ol>"},{"location":"odk-workflows/RepositoryFileStructure/#release-files","title":"Release files","text":"<p>Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here.</p>"},{"location":"odk-workflows/RepositoryFileStructure/#imports","title":"Imports","text":"<p>Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain.</p> <p>These are the current imports in CTO</p> Import URL Type nfdicore https://raw.githubusercontent.com/ISE-FIZKarlsruhe/nfdicore/main/nfdicore.ttl mirror schema https://raw.githubusercontent.com/schemaorg/schemaorg/refs/tags/v28.1-release/data/releases/28.1/schemaorg.owl custom skos http://www.w3.org/TR/skos-reference/skos.rdf slme ## Components Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: <ol> <li>There is an automated process that generates and re-generates a part of the ontology</li> <li>A part of the ontology is managed in ROBOT templates</li> <li>The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component.</li> </ol>"},{"location":"odk-workflows/SettingUpDockerForODK/","title":"Setting up your Docker environment for ODK use","text":"<p>One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception,  but more often than not it will appear as something like an <code>Error 137</code>. There are two places you need to consider to set your memory:</p> <ol> <li>Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding  <code>robot_java_args: '-Xmx8G'</code> to your src/ontology/cto-odk.yaml file, see for example here.</li> <li>Set your docker memory. By default, it should be about 10-20% more than your <code>robot_java_args</code> variable. You can manage your memory settings by right-clicking on the docker whale in your system bar--&gt;Preferences--&gt;Resources--&gt;Advanced, see picture below.</li> </ol> <p></p>"},{"location":"odk-workflows/UpdateImports/","title":"Update Imports Workflow","text":"<p>This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here.</p>"},{"location":"odk-workflows/UpdateImports/#importing-a-new-term","title":"Importing a new term","text":"<p>Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\".</p> <p>Importing a new term is split into two sub-phases:</p> <ol> <li>Declaring the terms to be imported</li> <li>Refreshing imports dynamically</li> </ol>"},{"location":"odk-workflows/UpdateImports/#declaring-terms-to-be-imported","title":"Declaring terms to be imported","text":"<p>There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be):</p> <ol> <li>Prot\u00e9g\u00e9-based declaration</li> <li>Using term files</li> <li>Using the custom import template</li> </ol>"},{"location":"odk-workflows/UpdateImports/#protege-based-declaration","title":"Prot\u00e9g\u00e9-based declaration","text":"<p>This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container.  This approach also applies to ontologies that use base module import approach.</p> <ol> <li>Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+).</li> <li>Select 'owl:Thing'</li> <li>Add a new class as usual.</li> <li>Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906.</li> <li>Click 'OK'</li> </ol> <p></p> <p>Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology.</p>"},{"location":"odk-workflows/UpdateImports/#using-term-files","title":"Using term files","text":"<p>Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in <code>src/ontology/go_import.owl</code>, you will also have an associated term file <code>src/ontology/go_terms.txt</code>. You can add terms in there simply as a list:</p> <pre><code>GO:0008150\nGO:0008151\n</code></pre> <p>Now you can run the refresh imports workflow) and the two terms will be imported.</p>"},{"location":"odk-workflows/UpdateImports/#using-the-custom-import-template","title":"Using the custom import template","text":"<p>This workflow is appropriate if:</p> <ol> <li>You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above).</li> <li>You wish to augment your imported ontologies with additional information. This requires a cautionary discussion.</li> </ol> <p>To enable this workflow, you add the following to your ODK config file (<code>src/ontology/cto-odk.yaml</code>), and update the repository:</p> <pre><code>use_custom_import_module: TRUE\n</code></pre> <p>Now you can manage your imported terms directly in the custom external terms template, which is located at <code>src/templates/external_import.owl</code>. Note that this file is a ROBOT template, and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully.</p> <p>The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import <code>APOLLO_SV:00000480</code>, and you wish to import <code>APOLLO_SV:00000532</code>, you simply add a row like this:</p> <pre><code>ID  Entity Type\nID  TYPE\nAPOLLO_SV:00000480  owl:Class\nAPOLLO_SV:00000532  owl:Class\n</code></pre> <p>When the imports are refreshed see imports refresh workflow, the term(s) will simply be imported from the configured ontologies.</p> <p>Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the <code>ID</code> and <code>ENTITY</code> columns and (b) ensure that the ROBOT template is valid otherwise, see here.</p> <p>WARNING. Note that doing this is a widespread antipattern (see related issue). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file.  </p>"},{"location":"odk-workflows/UpdateImports/#refresh-imports","title":"Refresh imports","text":"<p>If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example):</p> <p>First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory).  <pre><code>cd src/ontology\n</code></pre></p> <p>Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed.</p> <pre><code>sh run.sh make PAT=false imports/go_import.owl -B\n</code></pre> <p>Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above:</p> <pre><code>sh run.sh make refresh-go\n</code></pre> <p>Note that in case you changed the defaults, you need to add <code>IMP=true</code> and/or <code>MIR=true</code> to the command below:</p> <pre><code>sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B\n</code></pre> <p>If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. <code>go.owl</code> for your go import) you can set <code>MIR=false</code> instead, which will do the exact same thing as the above, but is easier to remember:</p> <pre><code>sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B\n</code></pre>"},{"location":"odk-workflows/UpdateImports/#using-the-base-module-approach","title":"Using the Base Module approach","text":"<p>Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it.  A base file is a subset of the ontology that only contains those axioms that nominally  belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this:</p> <p>Imagine this being the full Uberon ontology:</p> <pre><code>Axiom 1: BFO:123 SubClassOf BFO:124\nAxiom 1: UBERON:123 SubClassOf BFO:123\nAxiom 1: UBERON:124 SubClassOf UBERON 123\n</code></pre> <p>The base file is the set of all axioms that are about UBERON terms:</p> <pre><code>Axiom 1: UBERON:123 SubClassOf BFO:123\nAxiom 1: UBERON:124 SubClassOf UBERON 123\n</code></pre> <p>I.e.</p> <pre><code>Axiom 1: BFO:123 SubClassOf BFO:124\n</code></pre> <p>Gets removed.</p> <p>The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first  merging all mirrors into one huge file and then extracting one mega module from it.</p> <p>Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we</p> <p>1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module <code>imports/merged_import.owl</code></p> <p>The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml.</p> <p>To check if your ontology uses this method, check src/ontology/cto-odk.yaml to see if <code>use_base_merging: TRUE</code> is declared under <code>import_group</code></p> <p>If your ontology uses Base Module approach, please use the following steps: </p> <p>First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you)</p> <p>Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory).  <pre><code>cd src/ontology\n</code></pre></p> <p>Then refresh imports by running</p> <p><pre><code>sh run.sh make imports/merged_import.owl\n</code></pre> Note: if your mirrors are updated, you can run <code>sh run.sh make no-mirror-refresh-merged</code></p> <p>This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you.</p> <p>Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.</p>"},{"location":"odk-workflows/components/","title":"Adding components to an ODK repo","text":"<p>For details on what components are, please see component section of repository file structure document.</p> <p>To add custom components to an ODK repo, please follow the following steps:</p> <p>1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/cto-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component:</p> <pre><code>components:\n  products:\n    - filename: your-component-name.owl\n</code></pre> <p>3) Refresh your repo by running <code>sh run update_repo</code>. This will automatically (1) create a new file in <code>src/ontology/components/</code>, (2) update the <code>-edit</code> file so that it imports <code>https://nfdi4culture.de/ontology/cto/components/your-component-name.owl</code> (the IRI of your new component), and (3) update the XML catalog file (<code>src/ontology/catalog-v001.xml</code>) to redirect that IRI to the file in the <code>src/ontology/components</code> directory, so that the new component can be found by tools such as Prot\u00e9g\u00e9 or ROBOT, when they load the <code>-edit</code> file.</p> <p>If your component is to be generated by some automated process, add a goal in your custom Makefile (<code>src/ontology/cto.Makefile</code>) and make it perform any task needed to generate the component:</p> <pre><code>$(COMPONENTSDIR)/your-component-name.owl: $(SRC)\n    &lt;Insert here the code to produce the component&gt;\n</code></pre> <p>If the component is to be generated from a ROBOT template, the ODK can generate the appropriate code for you. For that, when adding the component fo the ODK configuration file (step 2 above), explicitly indicate that the component should be derived from template(s) and list the source templates:</p> <pre><code>components:\n  products:\n    - filename: your-component-name.owl\n      use_template: true\n      templates:\n        - template1.tsv\n        - template2.tsv\n</code></pre> <p>In this example, the component will be derived from the templates found in <code>src/templates/template1.tsv</code> and <code>src/templates/template2.tsv</code>. Initial empty templates will automatically be generated when the repository is refreshed (step 3).</p> <p>Likewise, the ODK can generate the required code for the case where the component is to be derived from SSSOM mappings:</p> <pre><code>components:\n  products:\n    - filename: your-component-name.owl\n      use_mappings: true\n      mappings:\n        - my-mappings.sssom.tsv\n</code></pre> <p>and for the case where the component is to be fetched from a remote resource:</p> <pre><code>components:\n  products:\n    - filename: your-component-name.owl\n      source: https://example.org/component-source.owl\n</code></pre>"},{"location":"widoco/readme/","title":"About Widoco output","text":"<p>The purpose of Widoco is to reuse and integrate existing tools for documentation, plus the set of features listed below: * Separation of the sections of your html page so you can write them independently and replace only those needed. * Automatic annotation in RDF-a of the html produced. * Association of a provenance page which includes the history of your vocabulary (W3C PROV-O compliant). * Metadata extraction from the ontology plus the means to complete it on the fly when generating your ontology. * Guidelines on the main sections that your document should have and how to complete them.</p> <p>Widoco will create 3 different folders: | |-provenance (a folder including an html and RDF serialization of how the documentation page was created) |-resources (folder with the different resources) |-sections (folder with the different sections of the documentation, separated for easy editing. Just edit one and the main page will be updated)</p>"},{"location":"widoco/readme/#completing-ontology-metadata","title":"Completing ontology metadata.","text":"<p>Widoco uses the ontology metadata to update a configuration file. If you complete that configuration file (ended up widoco.conf), the tool will enhance your html with additional details, such as how to cite the document, previous revisions, icons with the licence, etc.</p>"},{"location":"widoco/readme/#browser-issues","title":"Browser issues","text":"<p>The result of executing Widoco is an html file. We have tested it in Mozilla, IE and Chrome, and when the page is stored in a server all the browsers work correctly. If you view the file locally, we recommend you to use Mozilla Firefox (or Internet Explorer, if you must). Google Chrome will not show the contents correctly, as it doesn't allow  XMLHttpRequest without HTTP. If you want to view the page locally with Google Chrome you have two possibilities:</p> <p>a) Place the file in a server and access it via its URL (for example, put it in dropbox and access through its public url).</p> <p>b) Execute Chrome with the following commands :</p> <p>(WIN) chrome.exe --allow-file-access-from-files,</p> <p>(OSX) open /Applications/Google Chrome.app/ --args --allow-file-access-from-files</p> <p>(UNX) /usr/bin/google-chrome --allow-file-access-from-files</p> <p>Do you have a problem? open an issue at https://github.com/dgarijo/Widoco</p>"}]}